{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING! THIS FILE TOOK MY COMPUTER 2 DAYS TO RUN SO I HAD TO BREAK IT DOWN INTO STAGES\n",
    "\n",
    "Also probs wont run as github wouldnt let me upload like 40Gb of data. check readme doc for more info\n",
    "\n",
    "for processing each taxi type, one month of each data will take the follwing amounts to time\n",
    "- green and fhv are pretty small so its fast, like 4mins each\n",
    "- yellow takes 10 mins per data set \n",
    "- high volume fhv takes 1 hour per data set (times 24 for 2 years worth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ██████╗  ██████╗  ███╗   ██╗ ███████╗ ██╗  ██████╗  ██╗   ██╗ ██████╗   █████╗  ████████╗ ██╗  ██████╗  ███╗   ██╗ \n",
    "#██╔════╝ ██╔═══██╗ ████╗  ██║ ██╔════╝ ██║ ██╔════╝  ██║   ██║ ██╔══██╗ ██╔══██╗ ╚══██╔══╝ ██║ ██╔═══██╗ ████╗  ██║ \n",
    "#██║      ██║   ██║ ██╔██╗ ██║ █████╗   ██║ ██║  ███╗ ██║   ██║ ██████╔╝ ███████║    ██║    ██║ ██║   ██║ ██╔██╗ ██║ \n",
    "#██║      ██║   ██║ ██║╚██╗██║ ██╔══╝   ██║ ██║   ██║ ██║   ██║ ██╔══██╗ ██╔══██║    ██║    ██║ ██║   ██║ ██║╚██╗██║ \n",
    "#╚██████╗ ╚██████╔╝ ██║ ╚████║ ██║      ██║ ╚██████╔╝ ╚██████╔╝ ██║  ██║ ██║  ██║    ██║    ██║ ╚██████╔╝ ██║ ╚████║ \n",
    "# ╚═════╝  ╚═════╝  ╚═╝  ╚═══╝ ╚═╝      ╚═╝  ╚═════╝   ╚═════╝  ╚═╝  ╚═╝ ╚═╝  ╚═╝    ╚═╝    ╚═╝  ╚═════╝  ╚═╝  ╚═══╝ \n",
    "# global variables that are chagned often\n",
    "\n",
    "#█▀▀ █▀▀ █▄ █ █▀▀ █▀█ ▄▀█ █                                              HEY MARKER READ THE README THEN  |\n",
    "#█▄█ ██▄ █ ▀█ ██▄ █▀▄ █▀█ █▄▄                                               TO SPEED STUFF UP DO THIS    \\|/\n",
    "#\n",
    "#variable for skipping all the plotting code for debugging processing\n",
    "plotting = True\n",
    "\n",
    "# chose which taxi types to be processed\n",
    "process_ywl = True                                                                 # SET TO FALSE\n",
    "process_grn = True                                                                 # SET TO TRUE\n",
    "process_fhv = True                                                                 # SET TO TRUE\n",
    "process_hvf = True                                                                 # SET TO FALSE\n",
    "\n",
    "# limits data size and processes to be quicker (for testing only)\n",
    "testing = False                                                                    # SET TO TRUE\n",
    "start_month = 2        # testing var default 2\n",
    "number_of_months = 1   # testing var default 11\n",
    "start_day = 32         # testing var default 32ish\n",
    "number_of_days = 5     # testing var default 366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ██████╗  ██╗       ██████╗  ██████╗   █████╗  ██╗\n",
    "# ██╔════╝  ██║      ██╔═══██╗ ██╔══██╗ ██╔══██╗ ██║\n",
    "# ██║  ███╗ ██║      ██║   ██║ ██████╔╝ ███████║ ██║\n",
    "# ██║   ██║ ██║      ██║   ██║ ██╔══██╗ ██╔══██║ ██║\n",
    "# ╚██████╔╝ ███████╗ ╚██████╔╝ ██████╔╝ ██║  ██║ ███████╗\n",
    "#  ╚═════╝  ╚══════╝  ╚═════╝  ╚═════╝  ╚═╝  ╚═╝ ╚══════╝\n",
    "# ██╗   ██╗  █████╗  ██████╗  ███████╗\n",
    "# ██║   ██║ ██╔══██╗ ██╔══██╗ ██╔════╝\n",
    "# ██║   ██║ ███████║ ██████╔╝ ███████╗\n",
    "# ╚██╗ ██╔╝ ██╔══██║ ██╔══██╗ ╚════██║\n",
    "#  ╚████╔╝  ██║  ██║ ██║  ██║ ███████║\n",
    "#   ╚═══╝   ╚═╝  ╚═╝ ╚═╝  ╚═╝ ╚══════╝\n",
    "# v3.2.0\n",
    "# global variabels which can be tweeked\n",
    "\n",
    "# █▀▄ ▄▀█ ▀█▀ ▄▀█    █▀▄ █ █▀█\n",
    "# █▄▀ █▀█  █  █▀█    █▄▀ █ █▀▄\n",
    "#\n",
    "\n",
    "# main data directory\n",
    "download_dir = \"../../raw_data/\"\n",
    "\n",
    "#where the pre processed data goes\n",
    "processed_dir = \"../../processed_data/\" \n",
    "\n",
    "#where the tallying data goes\n",
    "tallied_dir = \"../../tallied_data/\" \n",
    "\n",
    "# intermediate file names\n",
    "processed_data_file = \"_processed_data_\"\n",
    "tally_s1_file = \"taxi_data_tallyed_s1.csv\"\n",
    "tally_s2_file = \"taxi_data_tallyed_s2.csv\"\n",
    "tally_s3_file = \"taxi_data_tallyed_s3.csv\"\n",
    "\n",
    "\n",
    "# █    ▀  █▀▀▄ █▀▀█ █▀▀█ █▀▀█ █  █ █▀▀\n",
    "# █   ▀█▀ █▀▀▄ █▄▄▀ █▄▄█ █▄▄▀ █▄▄█ ▀▀█\n",
    "# ▀▀▀ ▀▀▀ ▀▀▀  ▀ ▀▀ ▀  ▀ ▀ ▀▀ ▄▄▄█ ▀▀▀\n",
    "# imporint libarys used throughout\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import log, sqrt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import e\n",
    "\n",
    "import os.path\n",
    "import os\n",
    "from os.path import getsize\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import warnings\n",
    "\n",
    "# variable for skipping all the plotting code for debugging processing\n",
    "start_time = time.perf_counter()\n",
    "start_time_og = time.perf_counter()\n",
    "\n",
    "\n",
    "# ▀█▀ ▄▀█ ▀▄▀ █    █▀▄ ▄▀█ ▀█▀ ▄▀█\n",
    "#  █  █▀█ █ █ █    █▄▀ █▀█  █  █▀█\n",
    "#\n",
    "\n",
    "# file location\n",
    "file_base = \"_tripdata_\"\n",
    "\n",
    "# indexs for each of the main data set types\n",
    "ywl_i = 0\n",
    "grn_i = 1\n",
    "fhv_i = 2\n",
    "hvf_i = 3\n",
    "\n",
    "# used to identify which taxis to process\n",
    "process_taxi = [process_ywl, process_grn, process_fhv, process_hvf]\n",
    "\n",
    "# names related to each cab type in order\n",
    "taxi_file_names = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "full_names = [\"yellow cabs\", \"green cabs\",\n",
    "              \"for higher vheciles\", \"for higher high volume\"]\n",
    "\n",
    "#months in a year\n",
    "months_of_year = ['January', 'February', 'March', 'April', 'May',\n",
    "                  'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# column names for each cab type in order\n",
    "column_names = [\n",
    "    ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "     'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
    "     'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
    "     'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "     'total_amount', 'congestion_surcharge'],\n",
    "    ['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
    "     'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID',\n",
    "     'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax',\n",
    "     'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge',\n",
    "     'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge'],\n",
    "    ['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime',\n",
    "     'PULocationID', 'DOLocationID', 'SR_Flag'],\n",
    "    ['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime',\n",
    "     'dropoff_datetime', 'PULocationID', 'DOLocationID', 'SR_Flag']\n",
    "]\n",
    "\n",
    "# 2018 fhv files which was giving me trouble so added this schema for it\n",
    "bad_fhv_schema = [2, 2018, ['Pickup_DateTime', 'DropOff_datetime',\n",
    "                            'PULocationID', 'DOLocationID', 'SR_Flag',\n",
    "                            'dispatching_base_number']]\n",
    "\n",
    "\n",
    "# aliases for pickup time and dropoff time\n",
    "pickup_time = ['tpep_pickup_datetime', 'lpep_pickup_datetime',\n",
    "               'pickup_datetime', 'pickup_datetime', 'Pickup_DateTime']\n",
    "dropoff_time = ['tpep_dropoff_datetime', 'lpep_dropoff_datetime',\n",
    "                'dropoff_datetime', 'dropoff_datetime', 'DropOff_datetime']\n",
    "\n",
    "# irrelavent collumns that can be dropped\n",
    "irrelavent_columns = ['dispatching_base_number', 'DOtime', 'hvfhs_license_num',\n",
    "                      'dispatching_base_num', 'VendorID', 'store_and_fwd_flag',\n",
    "                      'extra', 'MTA_tax', 'improvement_surcharge',\n",
    "                      'tip_amount', 'tolls_amount', 'ehail_fee',\n",
    "                      'improvement_surcharge', 'congestion_surcharge']\n",
    "\n",
    "# array, for all the taxi count colls\n",
    "all_taxi_cols = [\n",
    "                \"2019_night\",\n",
    "                \"2019_morn\",\n",
    "                \"2019_arvo\",\n",
    "                \"2019_even\",\n",
    "                \"2020_night\",\n",
    "                \"2020_morn\",\n",
    "                \"2020_arvo\",\n",
    "                \"2020_even\"\n",
    "                ]\n",
    "\n",
    "# array for 2020 taxi count colls\n",
    "taxi_cols_2020 = [\n",
    "                 \"2020_night\",\n",
    "                 \"2020_morn\",\n",
    "                 \"2020_arvo\",\n",
    "                 \"2020_even\"]\n",
    "\n",
    "\n",
    "# ▀█▀ ▄▀█ ▀▄▀ █    █   █▀█ █▀▀ ▄▀█ ▀█▀ █ █▀█ █▄ █ █▀\n",
    "#  █  █▀█ █ █ █    █▄▄ █▄█ █▄▄ █▀█  █  █ █▄█ █ ▀█ ▄█\n",
    "#\n",
    "\n",
    "# location processing variables\n",
    "# locations that will be removed as considered invalid\n",
    "bad_locations = [1, 132, 138, 264, 265]\n",
    "\n",
    "# location of the location id file\n",
    "location_file = \"taxi+_zone_lookup.csv\"\n",
    "\n",
    "# list of boroughs names as in the location file\n",
    "boroughs_locations = [\"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\",\n",
    "                      \"Staten Island\", \"EWR\", \"Unknown\"]\n",
    "\n",
    "\n",
    "# █▀▄ █▀█ █ █ █ █▄ █ █   █▀█ ▄▀█ █▀▄\n",
    "# █▄▀ █▄█ ▀▄▀▄▀ █ ▀█ █▄▄ █▄█ █▀█ █▄▀\n",
    "#\n",
    "\n",
    "# taxi data base url\n",
    "taxi_url = f\"https://s3.amazonaws.com/nyc-tlc/trip+data/\"\n",
    "\n",
    "# covid cases\n",
    "covid_url = \"https://data.cityofnewyork.us/api/views/rc75-m7u3/rows.csv?accessType=DOWNLOAD\"\n",
    "\n",
    "# location data\n",
    "location_url = \"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "# █▀▄ ▄▀█ ▀█▀ █▀▀    █▀▄ █▀▀ █▀▀ █▀\n",
    "# █▄▀ █▀█  █  ██▄    █▄▀ ██▄ █▀  ▄█\n",
    "#\n",
    "months_in_year = 12\n",
    "\n",
    "# years of data used\n",
    "years = [2019, 2020]\n",
    "\n",
    "# feb 2019 was when fhvhv file started\n",
    "fhvhv_started = [2, 2019]\n",
    "\n",
    "# late feb 2020 was when the first case of covid was observed in NYC\n",
    "covid_started = [2, 2020]\n",
    "\n",
    "# preprocessing arrays\n",
    "\n",
    "# hours each day to seperate each count\n",
    "day_time_breakdown = [0, 6, 12, 18]\n",
    "\n",
    "# day of the year covid started on\n",
    "covid_start_day = int(datetime.datetime.strptime(\n",
    "    \" \".join([str(int) for int in covid_started]),\n",
    "    \"%m %Y\").__format__(\"%j\"))\n",
    "\n",
    "# pretty self explanitory\n",
    "days_in_2020 = 366\n",
    "\n",
    "\n",
    "# █▀█ █ █ ▀█▀ █   █ █▀▀ █▀█ █▀\n",
    "# █▄█ █▄█  █  █▄▄ █ ██▄ █▀▄ ▄█\n",
    "# values for calculating outliers\n",
    "\n",
    "# all vals must be withing the .99 percent of data\n",
    "outlier_percent = (1 - .99) / 2\n",
    "\n",
    "# buffer for graphing and removing outliers from graphs\n",
    "fare_amount_graph_buffer = 15\n",
    "\n",
    "# accuracy when calculating the percentile\n",
    "percentile_accuracy = 0.00001\n",
    "\n",
    "\n",
    "# █▀▀ █▀█ █ █ █ █▀▄    █▀▄ █▀▀ █▀▀ █▀\n",
    "# █▄▄ █▄█ ▀▄▀ █ █▄▀    █▄▀ ██▄ █▄▄ ▄█\n",
    "#\n",
    "\n",
    "# files\n",
    "covid_restrictions_file = \"covid_restrictions.csv\"\n",
    "covid_cases_file = \"COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\"\n",
    "\n",
    "# list of boroughs as named in the covid file (same order)\n",
    "covid_boroughs = [\"bx\", \"bk\", \"mn\", \"qn\", \"si\"]\n",
    "\n",
    "\n",
    "# █▀ █▀▀ ▀█▀ █ █ █▀█    █▀▀ █▀█ █▄ █ █▀▄ █ ▀█▀ █ █▀█ █▄ █ █▀\n",
    "# ▄█ ██▄  █  █▄█ █▀▀    █▄▄ █▄█ █ ▀█ █▄▀ █  █  █ █▄█ █ ▀█ ▄█\n",
    "#\n",
    "\n",
    "# plotting is using a yellow data set so must skip plotting if no yellow\n",
    "if plotting:\n",
    "    if not (process_taxi[ywl_i] and process_taxi[grn_i]):\n",
    "        plotting = False\n",
    "\n",
    "\n",
    "# if in a testing mode, sets up variables so file processes correctly\n",
    "if testing:\n",
    "\n",
    "    # changes the download dir to not overwrite data\n",
    "    # download_dir += \"../test_download/\"\n",
    "\n",
    "    # changes some timing variabels to anaylise only what is needed for testing\n",
    "    covid_started[0] = start_month\n",
    "    months_in_year = start_month + number_of_months - 1\n",
    "    graph_month = start_month\n",
    "    days_in_2020 = start_day + number_of_days\n",
    "    days_in_common = days_in_2020\n",
    "else:\n",
    "\n",
    "    # i not testing graphing month is 5 (so may)\n",
    "    graph_month = 5\n",
    "\n",
    "\n",
    "# █▀▄▀█ █ █▀ █▀▀    █▀▀ █ █ █▄ █ █▀▀\n",
    "# █ ▀ █ █ ▄█ █▄▄    █▀  █▄█ █ ▀█ █▄▄\n",
    "# specialized functions\n",
    "\n",
    "# sets seed for when its used\n",
    "seed = 1\n",
    "\n",
    "\n",
    "# log function to deal with integer count data, added conditions to set\n",
    "# any negative or 0 values to be 0\n",
    "def logf(x):\n",
    "    return (log(x) if x > 0 else 0)\n",
    "\n",
    "\n",
    "# applys a log log others 0\n",
    "def loglogf(x):\n",
    "    return (log(log(x)) if x > e else 0)\n",
    "\n",
    "\n",
    "# applys log log to an array\n",
    "def loglogfa(arr):\n",
    "    return [loglogf(x) for x in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup for plots to print large\n",
    "plt.show() \n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "plt.show() \n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xmx6000m\n",
      "Picked up _JAVA_OPTIONS: -Xmx6000m\n",
      "21/08/18 03:01:08 WARN Utils: Your hostname, DESKTOP-GN6J0KT resolves to a loopback address: 127.0.1.1; using 172.19.45.19 instead (on interface eth0)\n",
      "21/08/18 03:01:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/18 03:01:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#█▀ █▀█ ▄▀█ █▀█ █▄▀ \n",
    "#▄█ █▀▀ █▀█ █▀▄ █ █ \n",
    "# spark set up\n",
    "\n",
    "#removes warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# makes the outputted df nicely formatted\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#█▀▀ █▀▀ █  █ █▀▀ █▀▄▀█ █▀▀█    █▀▀ █▀▀ ▀▀█▀▀ █  █ █▀▀█ \n",
    "#▀▀█ █   █▀▀█ █▀▀ █ ▀ █ █▄▄█    ▀▀█ █▀▀   █   █  █ █▄▄█ \n",
    "#▀▀▀ ▀▀▀ ▀  ▀ ▀▀▀ ▀   ▀ ▀  ▀    ▀▀▀ ▀▀▀   ▀    ▀▀▀ █    \n",
    "# sets up the schema for each file type\n",
    "# this general code was taken from the spark tutorials in python folder\n",
    "# made by akira\n",
    "\n",
    "# grouping column names into their respective data type\n",
    "ints = ('SR_Flag','VendorID', 'trip_type', 'passenger_count', 'RateCodeID', 'RatecodeID','payment_type','PULocationID','DOLocationID')\n",
    "doubles = ('trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude','ehail_fee',\n",
    "           'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount','congestion_surcharge')\n",
    "strings = ('dispatching_base_number','store_and_fwd_flag','dispatching_base_num','hvfhs_license_num')\n",
    "dtimes = ('DropOff_datetime','Pickup_DateTime','tpep_pickup_datetime', 'tpep_dropoff_datetime', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'pickup_datetime', 'dropoff_datetime')\n",
    "\n",
    "#converting the types and col names into a dictionary\n",
    "dtypes = {column: IntegerType() for column in ints}\n",
    "dtypes.update({column: DoubleType() for column in doubles})\n",
    "dtypes.update({column: StringType() for column in strings})\n",
    "dtypes.update({column: TimestampType() for column in dtimes})\n",
    "\n",
    "\n",
    "#generating the schema\n",
    "schema = []\n",
    "\n",
    "#for each taxi type\n",
    "for index in range(0,len(taxi_file_names)):\n",
    "    \n",
    "    #creates a blank schema\n",
    "    schema.append(StructType())\n",
    "    \n",
    "    #adds datatype for each column\n",
    "    for column in column_names[index]:\n",
    "        schema[index].add(column,  # column name\n",
    "                   dtypes[column], # data type\n",
    "                   True            # is nullable?\n",
    "                  )\n",
    "        \n",
    "#schema for the bad fhv file\n",
    "bad_fhv_schema.append(StructType())\n",
    "\n",
    "#creates schema for each column of the badn schema files\n",
    "for column in bad_fhv_schema[2]:\n",
    "    bad_fhv_schema[3].add(column,\n",
    "                         dtypes[column],\n",
    "                         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping  yellow    \t2019-02\n",
      "importing green    \t2019-02\n",
      "importing fhv    \t2019-02\n",
      "skipping  fhvhv    \t2019-02\n",
      "skipping  yellow    \t2020-02\n",
      "importing green    \t2020-02\n",
      "importing fhv    \t2020-02\n",
      "skipping  fhvhv    \t2020-02\n"
     ]
    }
   ],
   "source": [
    "#██╗ ███╗   ███╗ ██████╗   ██████╗  ██████╗  ████████╗ ██╗ ███╗   ██╗  ██████╗  \n",
    "#██║ ████╗ ████║ ██╔══██╗ ██╔═══██╗ ██╔══██╗ ╚══██╔══╝ ██║ ████╗  ██║ ██╔════╝  \n",
    "#██║ ██╔████╔██║ ██████╔╝ ██║   ██║ ██████╔╝    ██║    ██║ ██╔██╗ ██║ ██║  ███╗ \n",
    "#██║ ██║╚██╔╝██║ ██╔═══╝  ██║   ██║ ██╔══██╗    ██║    ██║ ██║╚██╗██║ ██║   ██║ \n",
    "#██║ ██║ ╚═╝ ██║ ██║      ╚██████╔╝ ██║  ██║    ██║    ██║ ██║ ╚████║ ╚██████╔╝ \n",
    "#╚═╝ ╚═╝     ╚═╝ ╚═╝       ╚═════╝  ╚═╝  ╚═╝    ╚═╝    ╚═╝ ╚═╝  ╚═══╝  ╚═════╝  \n",
    "# imports all the relavent csv files to \n",
    "\n",
    "\n",
    "#local data dir for testing\n",
    "#download_dir = \"/mnt/e/2021/Applied Data Science/Project 1/Data/\" \n",
    "\n",
    "#▀█▀ ▄▀█ ▀▄▀ █ \n",
    "# █  █▀█ █ █ █ \n",
    "# function for reading an individual taxi file\n",
    "def read_csv(year, month, taxi_i, schema_used, drop_index):\n",
    "    return (\n",
    "        spark.read.csv(\n",
    "            download_dir + taxi_file_names[taxi_i] + file_base + str(year) + \"-\" + str(month).zfill(2) + \".csv\", \n",
    "            header=True, \n",
    "            schema=schema_used)\\\n",
    "        .withColumnRenamed(pickup_time[drop_index],\"PUtime\")# rename the time col\n",
    "        .withColumnRenamed(dropoff_time[drop_index],\"DOtime\")# rename other time col\n",
    "        .drop(*irrelavent_columns)\n",
    "    )\n",
    "\n",
    "\n",
    "#spark data frame matrix\n",
    "dfm = {}\n",
    "\n",
    "#for each year\n",
    "for year in years:\n",
    "    dfm[year] = {}\n",
    "    \n",
    "    #loops over the months in years\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        dfm[year][month] = {}\n",
    "    \n",
    "        #for each taxi variant\n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            dfm[year][month][taxi_i] = 0\n",
    "           \n",
    "            #if the current taxi year month combo is a bad schema, fixes that\n",
    "            if(bad_fhv_schema[1] == year and taxi_i == fhv_i):\n",
    "                \n",
    "                schema_used = bad_fhv_schema[3]\n",
    "                drop_index = 4\n",
    "            else:\n",
    "                \n",
    "                #otherwise uses the default for the taxi type\n",
    "                schema_used = schema[taxi_i]\n",
    "                drop_index = taxi_i\n",
    "                \n",
    "                    \n",
    "            # if we need to proccess this taxi plus small condition as fhvhv didnt always exist\n",
    "            if(process_taxi[taxi_i] and ((month >= fhvhv_started[0] and year >= fhvhv_started[1]) or taxi_i != hvf_i)):\n",
    "                \n",
    "                #prints that the functions is processing\n",
    "                print(\"importing \" + taxi_file_names[taxi_i] + \"    \\t\" + str(year) + \"-\" + str(month).zfill(2))                \n",
    "                \n",
    "                #reads the first dataframe\n",
    "                dfm[year][month][taxi_i] = read_csv(year, month, taxi_i, schema_used, drop_index)\n",
    "                \n",
    "                #limits data size if testing\n",
    "                if testing:\n",
    "                    dfm[year][month][taxi_i] = dfm[year][month][taxi_i].sample(False, 0.1, seed=seed).limit(10000)\n",
    "            \n",
    "            else:\n",
    "                #prints what file is being skipped\n",
    "                print(\"skipping  \" + taxi_file_names[taxi_i] + \"    \\t\" + str(year) + \"-\" + str(month).zfill(2))   \n",
    "                \n",
    "                #sets array to be null if its not being processed\n",
    "                dfm[year][month][taxi_i] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for  2019-02 counts were:\n",
      "10000\n",
      "10000\n",
      "for  2020-02 counts were:\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#counting the size of the data\n",
    "\n",
    "#for each year, month and each taxi \n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        \n",
    "        #sets counts to 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        \n",
    "        #for each taxi\n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            \n",
    "            #checks the data frame exists\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #adds count to the respective count\n",
    "                if(taxi_i in [0,1]):\n",
    "                    count1 += dfm[year][month][taxi_i].count()\n",
    "                else:\n",
    "                    count2 += dfm[year][month][taxi_i].count()\n",
    "            \n",
    "        #prints counts\n",
    "        print(\"for  \" + str(year) + \"-\" + str(month).zfill(2) + \" counts were:\")  \n",
    "        print(count1)\n",
    "        print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.445282000000589 seconds have elapsed\n",
      "14.445431899999676 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "#prints how much time has elapsed\n",
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#█▀█ █   █▀█ ▀█▀ ▀█▀ █ █▄ █ █▀▀ \n",
    "#█▀▀ █▄▄ █▄█  █   █  █ █ ▀█ █▄█ \n",
    "# Functions used to plot the data and check it sup to scratch\n",
    "\n",
    "#extracts necessary data for graphs and trasnforms it\n",
    "def extract_graph_data(dfm, columns_to_contrast):\n",
    "    \n",
    "    #sets arrays to read data\n",
    "    data = {}\n",
    "    data_log = {}\n",
    "    \n",
    "    #for both plottable taxi types\n",
    "    for taxi_i in [ywl_i, grn_i]:\n",
    "        \n",
    "        #gets some data to plot, dont want too much\n",
    "        data[taxi_i] = dfm[years[0]][graph_month][taxi_i]\\\n",
    "            .sample(False, 0.1, seed=0)\\\n",
    "            .limit(10000)\\\n",
    "            .select(columns_to_contrast)\\\n",
    "            .toPandas()\n",
    "\n",
    "        #makes a copy of the dataframe to be logged\n",
    "        data_log[taxi_i]  = data[taxi_i].copy()\n",
    "        \n",
    "        #log transforms each column\n",
    "        for column in columns_to_contrast:\n",
    "            data_log[taxi_i][column] = data_log[taxi_i][column].apply(lambda x: log(x) if x else 0)\n",
    "    \n",
    "        \n",
    "    return [data, data_log]\n",
    "    \n",
    "#creates a graph of fare fare amount vs trip distance\n",
    "def graph_fare_vs_trip(data, taxi_i, columns_to_contrast, ax):\n",
    "    \n",
    "        #creates a scatter plot\n",
    "        data[taxi_i].plot.scatter(\n",
    "                          y = columns_to_contrast[0],\n",
    "                          x = columns_to_contrast[1], \n",
    "                          ax = ax,\n",
    "                          s=75\n",
    "                         )\n",
    "        \n",
    "        #labels the graph\n",
    "        ax.set_title(full_names[taxi_i], fontsize=20)\n",
    "        ax.set(ylabel = \"log of \" + columns_to_contrast[0])\n",
    "        ax.set(xlabel = \"log of \" + columns_to_contrast[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def graph_histogram(data, taxi_i, columns_to_contrast, ax, column_index, x_label_prefix = \"log of \", log_i = 1):\n",
    "    #turns the index into a string, index is also needed for graphing\n",
    "    column = columns_to_contrast[column_index]\n",
    "    \n",
    "    #x values are the column\n",
    "    x_vals = data[taxi_i][column]\n",
    "    \n",
    "    \n",
    "    #buffer size is scaled if data log transformed\n",
    "    if log_i:\n",
    "        buffer = log(fare_amount_graph_buffer)\n",
    "    else:\n",
    "        buffer = fare_amount_graph_buffer\n",
    "        \n",
    "        \n",
    "    #finds the range to graph on\n",
    "    x_range = (x_vals.quantile(outlier_percent) - buffer,\n",
    "               x_vals.quantile(1-outlier_percent) + buffer)\n",
    "       \n",
    "    #creates the binnning for the graph\n",
    "    bins = round((x_vals.max() - x_vals.min()) \n",
    "                 * bin_scaling[column_index][log_i])\n",
    "                \n",
    "    #adds in the graph\n",
    "    sns.distplot(x_vals, bins = bins, ax = ax)                \n",
    "    ax.set_xlim(x_range)\n",
    "    \n",
    "    #labels the graph\n",
    "    ax.set_title(x_label_prefix + column + \" histogram\", fontsize=20)\n",
    "    ax.set(ylabel = \"Density\")\n",
    "    ax.set(xlabel = x_label_prefix + column)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#function for seeing how filtering looks\n",
    "#graphs fare vs trip data\n",
    "def fare_vs_trip_and_hist(dfm, filter_level_subtitle):\n",
    "        \n",
    "    #whcih two columns are we plotting \n",
    "    columns_to_contrast = ['fare_amount','trip_distance']\n",
    "        \n",
    "    #reads log data\n",
    "    data = extract_graph_data(dfm, columns_to_contrast)[1]\n",
    "    \n",
    "    #creates 4 sub plots\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "        \n",
    "    #adds main title\n",
    "    fig.suptitle(months_of_year[graph_month] + \" \"+ str(years[0]) + \" \" + full_names[ywl_i] + \" \\n\" +\n",
    "                \"Trip distance and Fare amount, \" + filter_level_subtitle\n",
    "                 , fontsize = 30)\n",
    "        \n",
    "        \n",
    "    #for each subplot\n",
    "    for graph_col in [0,1]:\n",
    "        for graph_row in [0, 1]:\n",
    "            \n",
    "            ax = axs[graph_row, graph_col]\n",
    "            \n",
    "            #logs are in second column:\n",
    "            if graph_col:\n",
    "                \n",
    "                \n",
    "                #creates scatter plots\n",
    "                graph_fare_vs_trip(data, graph_row, columns_to_contrast, ax)\n",
    "            \n",
    "            #histograms are in the first column:\n",
    "            else:\n",
    "            \n",
    "                #creates histograms\n",
    "                graph_histogram(data, ywl_i, columns_to_contrast, ax, graph_row)\n",
    "\n",
    "        \n",
    "        \n",
    "    #labels the graph as a whole\n",
    "    for ax in axs.flat:\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(20)\n",
    "        #ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks how the inital data looks\n",
    "\n",
    "#scaling for bins\n",
    "bin_scaling = [[1/3, 3], [1, 3]]\n",
    "\n",
    "if plotting:\n",
    "    \n",
    "    #whcih two columns are we plotting \n",
    "    columns_to_contrast = ['fare_amount','trip_distance']\n",
    "    \n",
    "    #extracting all the relaven data\n",
    "    data = extract_graph_data(dfm, columns_to_contrast)\n",
    "    \n",
    "    #creates 4 sub plots\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    \n",
    "    fig.suptitle(months_of_year[graph_month] + \" \"+ str(years[0]) + \" \" + full_names[ywl_i] + \" \\n\" +\n",
    "                \"Trip distance and Fare amount histograms, no filtering\"\n",
    "                 , fontsize = 30)\n",
    "    \n",
    "    #for each combo of log/ linear and fare/ distance\n",
    "    for fare_i in [0,1]:\n",
    "        for log_i in [0, 1]:\n",
    "            \n",
    "            ax = axs[log_i, fare_i]\n",
    "            \n",
    "            #creates histograms\n",
    "            graph_histogram(\n",
    "                data[log_i], \n",
    "                ywl_i, \n",
    "                columns_to_contrast, \n",
    "                ax, \n",
    "                fare_i, \n",
    "                x_label_prefix = [\"\",\"log of \"][log_i],\n",
    "                log_i = log_i\n",
    "            )\n",
    "            \n",
    "            if fare_i == 1:\n",
    "                ax.set(ylabel = \"Density\")\n",
    "            else:\n",
    "                ax.set(ylabel = \"\")\n",
    "            ax.set(xlabel = [\"\",\"log of \"][log_i] + column)\n",
    "        \n",
    "    #labels the graph as a whole\n",
    "    for ax in axs.flat:\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(20)\n",
    "        #ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if plotting:\n",
    "    #creates 4 sub plots\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    \n",
    "    #creates main title\n",
    "    fig.suptitle(months_of_year[graph_month] + \" \"+ str(years[0]) + \", \" +\n",
    "                \"Trip distance vs Fare amount, no filtering\"\n",
    "                 , fontsize = 30)\n",
    "    \n",
    "    fig.set_size_inches(15, 7.5)\n",
    "    \n",
    "    #reads log data\n",
    "    data = extract_graph_data(dfm, columns_to_contrast)[1]\n",
    "    \n",
    "    #for each taxi type to graph\n",
    "    for taxi_i in [ywl_i, grn_i]:\n",
    "        \n",
    "        #creates graphs\n",
    "        graph_fare_vs_trip(data, taxi_i, columns_to_contrast, axs[taxi_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks how the inital data looks\n",
    "\n",
    "#creates graph of fare vs trip \n",
    "if plotting:\n",
    "    fare_vs_trip_and_hist(dfm, \"no filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 2019-2 green\n",
      "processing 2019-2 fhv\n",
      "processing 2020-2 green\n",
      "processing 2020-2 fhv\n"
     ]
    }
   ],
   "source": [
    "#███████╗ ██╗ ██╗      ████████╗ ███████╗ ██████╗  ██╗ ███╗   ██╗  ██████╗  \n",
    "#██╔════╝ ██║ ██║      ╚══██╔══╝ ██╔════╝ ██╔══██╗ ██║ ████╗  ██║ ██╔════╝  \n",
    "#█████╗   ██║ ██║         ██║    █████╗   ██████╔╝ ██║ ██╔██╗ ██║ ██║  ███╗ \n",
    "#██╔══╝   ██║ ██║         ██║    ██╔══╝   ██╔══██╗ ██║ ██║╚██╗██║ ██║   ██║ \n",
    "#██║      ██║ ███████╗    ██║    ███████╗ ██║  ██║ ██║ ██║ ╚████║ ╚██████╔╝ \n",
    "#╚═╝      ╚═╝ ╚══════╝    ╚═╝    ╚══════╝ ╚═╝  ╚═╝ ╚═╝ ╚═╝  ╚═══╝  ╚═════╝  \n",
    "# removing values that dont make sense, etc\n",
    "\n",
    "\n",
    "#█   █▀█ █▀▀ ▄▀█ ▀█▀ █ █▀█ █▄ █ \n",
    "#█▄▄ █▄█ █▄▄ █▀█  █  █ █▄█ █ ▀█ \n",
    "# removing invalid locations\n",
    "\n",
    "#checks if location id is in the bad location list\n",
    "@F.udf(\"boolean\")\n",
    "def bad_location(LocationID):\n",
    "    \n",
    "    #if it has a location\n",
    "    if(LocationID):\n",
    "        \n",
    "        #checks if its valid\n",
    "        return (int(LocationID) not in bad_locations)\n",
    "    else:\n",
    "        \n",
    "        #otherwise theres no locaiton so its invalid\n",
    "        return False\n",
    "\n",
    "    \n",
    "#for each year, month and each taxi \n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            \n",
    "            #checks the data frame exists\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #prints processing\n",
    "                print(\"processing \" + str(year) + \"-\" + str(month) + \" \" + taxi_file_names[taxi_i])\n",
    "                \n",
    "                #filters said dataframe for both bad pickup and dropoff locations\n",
    "                dfm[year][month][taxi_i] = dfm[year][month][taxi_i]\\\n",
    "                    .filter(\n",
    "                        (bad_location(col(\"PULocationID\"))) &\n",
    "                        (bad_location(col(\"DOLocationID\")))\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates graph of fare vs trip \n",
    "if plotting:\n",
    "    fare_vs_trip_and_hist(dfm, \"semi filtered with outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for  2019-02 counts were:\n",
      "9804\n",
      "883\n",
      "for  2020-02 counts were:\n",
      "9763\n",
      "1528\n"
     ]
    }
   ],
   "source": [
    "#counting the size of the data\n",
    "\n",
    "#for each year, month and each taxi \n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        \n",
    "        #sets counts to 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        \n",
    "        #for each taxi\n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            \n",
    "            #checks the data frame exists\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #adds count to the respective count\n",
    "                if(taxi_i in [0,1]):\n",
    "                    count1 += dfm[year][month][taxi_i].count()\n",
    "                else:\n",
    "                    count2 += dfm[year][month][taxi_i].count()\n",
    "            \n",
    "        #prints counts\n",
    "        print(\"for  \" + str(year) + \"-\" + str(month).zfill(2) + \" counts were:\")  \n",
    "        print(count1)\n",
    "        print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#█▄▄ ▄▀█ █▀ █ █▀▀    █▀▀ █ █ █ █    █▀▀ █ █   ▀█▀ █▀▀ █▀█ \n",
    "#█▄█ █▀█ ▄█ █ █▄▄    █▀  █▀█ ▀▄▀    █▀  █ █▄▄  █  ██▄ █▀▄ \n",
    "# filters fhv only\n",
    "\n",
    "#for each data frame that exisits\n",
    "for year in years:\n",
    "    for taxi_i in [fhv_i, hvf_i]:\n",
    "        for month in range(covid_started[0], months_in_year + 1): \n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "            \n",
    "                #keeps rows which dont have the SR_flag\n",
    "                dfm[year][month][taxi_i] = dfm[year][month][taxi_i]\\\n",
    "                .filter(col(\"SR_flag\").isNull())\\\n",
    "                .drop(\"SR_flag\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following processing only makes sense for yellow and green taxis\n",
    "\n",
    "#█▄▄ ▄▀█ █▀ █ █▀▀    ▀█▀ ▄▀█ ▀▄▀ █    █▀▀ █ █   ▀█▀ █▀▀ █▀█ \n",
    "#█▄█ █▀█ ▄█ █ █▄▄     █  █▀█ █ █ █    █▀  █ █▄▄  █  ██▄ █▀▄ \n",
    "# \n",
    "# removing invalid passenger counts\n",
    "# only keep standard rates\n",
    "# payment types which arent helpful\n",
    "\n",
    "#for each data frame that exisits\n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        for taxi_i in [ywl_i, grn_i]:\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #filters out rows which dont have standard values for the \n",
    "                #afore mentioned columns\n",
    "                dfm[year][month][taxi_i] = dfm[year][month][taxi_i]\\\n",
    "                .filter(\n",
    "                    (\n",
    "                        (col(\"payment_type\") == 1) | \n",
    "                        (col(\"payment_type\") == 0)\n",
    "                    ) &\n",
    "                    (col(\"passenger_count\") > 0) &\n",
    "                    (col(\"RateCodeID\") == 1)\n",
    "                )\n",
    "                 \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#█▀▀▄  ▀  █▀▀ ▀▀█▀▀    █▀▀█ █▀▀▄ █▀▀▄    █▀▀ █▀▀█ █▀▀ ▀▀█▀▀    █▀▀▀ █▀▀█ █▀▀█ █▀▀█ █  █  ▀  █▀▀▄ █▀▀▀ \n",
    "#█  █ ▀█▀ ▀▀█   █      █▄▄█ █  █ █  █    █   █  █ ▀▀█   █      █ ▀█ █▄▄▀ █▄▄█ █▄▄█ █▀▀█ ▀█▀ █  █ █ ▀█ \n",
    "#▀▀▀  ▀▀▀ ▀▀▀   ▀      ▀  ▀ ▀  ▀ ▀▀▀     ▀▀▀ ▀▀▀▀ ▀▀▀   ▀      ▀▀▀▀ ▀ ▀▀ ▀  ▀ █    ▀  ▀ ▀▀▀ ▀  ▀ ▀▀▀▀ \n",
    "# cost are sometimes bad, graphing what it looks like atm\n",
    "\n",
    "#creates graph of fare vs trip \n",
    "if plotting:\n",
    "    fare_vs_trip_and_hist(dfm, \"filtered with outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8549694000030286 seconds have elapsed\n",
      "18.300890899998194 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "#prints how much time has elapsed\n",
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined quantile:\n",
      "[[2.5, 0.0], [0.0, 0.0]]\n",
      "processing 2019-2 green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0, 68.0], [0.0, 21.85]]\n",
      "combined quantile:\n",
      "[[3.0, 68.0], [0.0, 21.85]]\n",
      "combined quantile:\n",
      "[[2.5, 0.0], [0.0, 0.0]]\n",
      "processing 2020-2 green\n",
      "[[3.5, 65.0], [0.0, 20.19]]\n",
      "combined quantile:\n",
      "[[3.5, 65.0], [0.0, 20.19]]\n"
     ]
    }
   ],
   "source": [
    "#█▀▄ █ █▀ ▀█▀    ▄▀█ █▄ █ █▀▄    █▀▀ █▀█ █▀ ▀█▀    █▀▀ █ █   ▀█▀ █▀▀ █▀█ \n",
    "#█▄▀ █ ▄█  █     █▀█ █ ▀█ █▄▀    █▄▄ █▄█ ▄█  █     █▀  █ █▄▄  █  ██▄ █▀▄ \n",
    "#\n",
    "#as fare amount is obviously dependent on trip distance, the one quantiles cant be calculated after processing \n",
    "#the other set so they must both be processed in the same filter\n",
    "\n",
    "#making quantile array\n",
    "qt = {}\n",
    "\n",
    "#for each year, create sub array\n",
    "for year in years:\n",
    "    qt[year] = {}\n",
    "    \n",
    "    #for each taxi create another sub array\n",
    "    for taxi_i in [ywl_i, grn_i]:\n",
    "        qt[year][taxi_i] = {}\n",
    "        \n",
    "        #creates combined quantile which will be used for averaging\n",
    "        qt_combined = [[0,0],[0,0]]\n",
    "        count = 0\n",
    "        \n",
    "        #for each month\n",
    "        for month in range(covid_started[0], months_in_year + 1): \n",
    "            \n",
    "            #set quantile as empty\n",
    "            qt[year][taxi_i][month] = None\n",
    "            \n",
    "            #checks data exists\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #prints processing update\n",
    "                print(\"processing \" + str(year) + \"-\" + str(month) + \" \" + taxi_file_names[taxi_i])\n",
    "                \n",
    "                #approximates the quantiles for the cost and distance\n",
    "                qt[year][taxi_i][month] = dfm[year][month][taxi_i].approxQuantile(\n",
    "                    [\"fare_amount\",'trip_distance'], \n",
    "                    [outlier_percent,1-outlier_percent], \n",
    "                    percentile_accuracy\n",
    "                )\n",
    "                \n",
    "                #adds values to the total\n",
    "                for i in range(0,2):\n",
    "                    for j in range(0,2):\n",
    "                        qt_combined[i][j] += qt[year][taxi_i][month][i][j]\n",
    "                    \n",
    "                count += 1\n",
    "                \n",
    "                #prints quantiles\n",
    "                print(qt[year][taxi_i][month])\n",
    "            \n",
    "            else:\n",
    "                #if the current taxi type inst being counted sets count to 1\n",
    "                #so no errors occur\n",
    "                count += 1\n",
    "                \n",
    "        #for each quantile, turns the sum into an avaerage\n",
    "        for i in range(0,2):\n",
    "            for j in range(0,2):\n",
    "                \n",
    "                #checks count exists\n",
    "                if(count):\n",
    "                    qt_combined[i][j] = qt_combined[i][j]/count\n",
    "\n",
    "        \n",
    "        #2.5 is the miniumum fare_amount so if the fare quantile was lower than the minimum, it is raised\n",
    "        if qt_combined[0][0] < 2.5 :\n",
    "            qt_combined[0][0] = 2.5\n",
    "                \n",
    "        #less than a 0km trip shouldnt be counted so min of distance it set to 0 if it was lower.\n",
    "        if qt_combined[1][0] < 0 :\n",
    "            qt_combined[1][0] = 0\n",
    "                         \n",
    "        #prints comvines quantile\n",
    "        print(\"combined quantile:\")\n",
    "        print(qt_combined)               \n",
    "            \n",
    "            \n",
    "        #for each month\n",
    "        for month in range(covid_started[0], months_in_year + 1): \n",
    "            \n",
    "            #checks data exists\n",
    "            if( dfm[year][month][taxi_i] ):                       \n",
    "            \n",
    "                #only keeps the middle percent% (depends on what percent was decided) to remove outliers \n",
    "                dfm[year][month][taxi_i] = dfm[year][month][taxi_i].filter(\n",
    "                    (col(\"fare_amount\") > qt_combined[0][0]) & \n",
    "                    (col(\"fare_amount\") < qt_combined[0][1]) &                \n",
    "                    (col('trip_distance') > qt_combined[1][0]) & \n",
    "                    (col('trip_distance') < qt_combined[1][1])\n",
    "                )#.drop(\"fare_amount\",\"total_amount\"'trip_distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for  2019-02 counts were:\n",
      "5018\n",
      "883\n",
      "for  2020-02 counts were:\n",
      "4943\n",
      "1528\n"
     ]
    }
   ],
   "source": [
    "#counting the size of the data\n",
    "\n",
    "#for each year, month and each taxi \n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        \n",
    "        #sets counts to 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        \n",
    "        #for each taxi\n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            \n",
    "            #checks the data frame exists\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #adds count to the respective count\n",
    "                if(taxi_i in [0,1]):\n",
    "                    count1 += dfm[year][month][taxi_i].count()\n",
    "                else:\n",
    "                    count2 += dfm[year][month][taxi_i].count()\n",
    "            \n",
    "        #prints counts\n",
    "        print(\"for  \" + str(year) + \"-\" + str(month).zfill(2) + \" counts were:\")  \n",
    "        print(count1)\n",
    "        print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2503319999959785 seconds have elapsed\n",
      "22.551527899995563 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "#prints how much time has elapsed\n",
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2019: {0: {2: None}, 1: {2: [[3.0, 68.0], [0.0, 21.85]]}}, 2020: {0: {2: None}, 1: {2: [[3.5, 65.0], [0.0, 20.19]]}}}\n"
     ]
    }
   ],
   "source": [
    "#prints quantiles incase they get lost\n",
    "print(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checks how the final filtering looks\n",
    "\n",
    "#creates graph of fare vs trip \n",
    "if plotting:\n",
    "    fare_vs_trip_and_hist(dfm, \"completely filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031092199998965953 seconds have elapsed\n",
      "22.58349779999844 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#██████╗  ██████╗  ███████╗ ██████╗  ██████╗   ██████╗   ██████╗ ███████╗ ███████╗ ███████╗ ██╗ ███╗   ██╗  ██████╗  \n",
    "#██╔══██╗ ██╔══██╗ ██╔════╝ ██╔══██╗ ██╔══██╗ ██╔═══██╗ ██╔════╝ ██╔════╝ ██╔════╝ ██╔════╝ ██║ ████╗  ██║ ██╔════╝  \n",
    "#██████╔╝ ██████╔╝ █████╗   ██████╔╝ ██████╔╝ ██║   ██║ ██║      █████╗   ███████╗ ███████╗ ██║ ██╔██╗ ██║ ██║  ███╗ \n",
    "#██╔═══╝  ██╔══██╗ ██╔══╝   ██╔═══╝  ██╔══██╗ ██║   ██║ ██║      ██╔══╝   ╚════██║ ╚════██║ ██║ ██║╚██╗██║ ██║   ██║ \n",
    "#██║      ██║  ██║ ███████╗ ██║      ██║  ██║ ╚██████╔╝ ╚██████╗ ███████╗ ███████║ ███████║ ██║ ██║ ╚████║ ╚██████╔╝ \n",
    "#╚═╝      ╚═╝  ╚═╝ ╚══════╝ ╚═╝      ╚═╝  ╚═╝  ╚═════╝   ╚═════╝ ╚══════╝ ╚══════╝ ╚══════╝ ╚═╝ ╚═╝  ╚═══╝  ╚═════╝  \n",
    "# Adding generated features and changing forms of other features\n",
    "\n",
    "#█▀▄ ▄▀█ ▀█▀ █▀▀    ▄▀█ █▄ █ █▀▄    ▀█▀ █ █▀▄▀█ █▀▀ \n",
    "#█▄▀ █▀█  █  ██▄    █▀█ █ ▀█ █▄▀     █  █ █ ▀ █ ██▄ \n",
    "# adding in binned date and time\n",
    "\n",
    "#UDFs can only have one output\n",
    "#this UDF bins the time of the day into the specified groups with int output between 1 - n being the group number\n",
    "@F.udf(\"int\")\n",
    "def find_part_of_day(time):\n",
    "    \n",
    "    #finds hour of the day once to speed code a bit\n",
    "    hour = int(time.__format__(\"%H\"))\n",
    "    \n",
    "    #checks which part of the day that hour should be in\n",
    "    if hour < day_time_breakdown[1]:\n",
    "        return 0\n",
    "    elif hour < day_time_breakdown[2]:\n",
    "        return 1\n",
    "    elif hour < day_time_breakdown[3]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "#this UDF extracts the week of the year from the date time column\n",
    "#years are captured in the matrix level and months will become irrelavent shortly\n",
    "@F.udf(\"int\")\n",
    "def find_week_of_year(time):\n",
    "    return int(time.__format__(\"%U\"))\n",
    "\n",
    "#the individual days can then be subsorted by looking at the day of the week\n",
    "@F.udf(\"int\")\n",
    "def find_day_of_week(time):\n",
    "    return int(time.__format__(\"%w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#█   █▀█ █▀▀ ▄▀█ ▀█▀ █ █▀█ █▄ █ \n",
    "#█▄▄ █▄█ █▄▄ █▀█  █  █ █▄█ █ ▀█ \n",
    "# adding in borough number\n",
    "\n",
    "#first we need to get the location id to borough id matrix \n",
    "\n",
    "#reads the location id file\n",
    "borough_codes = pd.read_csv(download_dir + location_file)\n",
    "\n",
    "#finds the 'id' for each location interms of the 5 boroughs\n",
    "codes = []\n",
    "for index, row in borough_codes.iterrows():\n",
    "    codes.append(boroughs_locations.index(row[\"Borough\"]))\n",
    "\n",
    "#adds said list to the dataframe    \n",
    "borough_codes['Borough_code'] = pd.Series(codes, index = borough_codes.index)\n",
    "\n",
    "#location \n",
    "@F.udf(\"int\")\n",
    "def find_borough_code(location):\n",
    "    return int(borough_codes.loc[borough_codes[\"LocationID\"] == int(location)][\"Borough_code\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 2019 green\n",
      "processing 2019 fhv\n",
      "processing 2020 green\n",
      "processing 2020 fhv\n"
     ]
    }
   ],
   "source": [
    "#█▀▀ █ █   ▀█▀ █▀▀ █▀█ █ █▄ █ █▀▀ \n",
    "#█▀  █ █▄▄  █  ██▄ █▀▄ █ █ ▀█ █▄█ \n",
    "# applys above filters\n",
    "\n",
    "#applying location and date row transformations\n",
    "\n",
    "#for each data frame that exists\n",
    "for year in years:\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #prints we are processing it\n",
    "                print(\"processing \" + str(year) + \" \" + taxi_file_names[taxi_i])\n",
    "                \n",
    "                #adds the generated features to respective columns\n",
    "                dfm[year][month][taxi_i] = dfm[year][month][taxi_i]\\\n",
    "                    .withColumn(\"PU_borough_code\", find_borough_code(col(\"PULocationID\")))\\\n",
    "                    .withColumn(\"DO_borough_code\", find_borough_code(col(\"DOLocationID\")))\\\n",
    "                    .withColumn(\"week_of_year\", find_week_of_year(col(\"PUtime\")))\\\n",
    "                    .withColumn(\"day_of_week\", find_day_of_week(col(\"PUtime\")))\\\n",
    "                    .withColumn(\"part_of_day\", find_part_of_day(col(\"PUtime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4226579000023776 seconds have elapsed\n",
      "23.00642130000051 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "#prints how much time has elapsed\n",
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting 2019-2 green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.838976399994863 seconds have elapsed\n",
      "30.859060099995986 seconds have elapsed total\n",
      "counting 2019-2 fhv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.153068299994629 seconds have elapsed\n",
      "35.012293499996304 seconds have elapsed total\n",
      "counting 2020-2 green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.967723099995055 seconds have elapsed\n",
      "40.98026140000002 seconds have elapsed total\n",
      "counting 2020-2 fhv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.851510300002701 seconds have elapsed\n",
      "45.832583199997316 seconds have elapsed total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ██████╗  ██████╗  ██╗   ██╗ ███╗   ██╗ ████████╗ ██╗ ███╗   ██╗  ██████╗  \n",
    "#██╔════╝ ██╔═══██╗ ██║   ██║ ████╗  ██║ ╚══██╔══╝ ██║ ████╗  ██║ ██╔════╝  \n",
    "#██║      ██║   ██║ ██║   ██║ ██╔██╗ ██║    ██║    ██║ ██╔██╗ ██║ ██║  ███╗ \n",
    "#██║      ██║   ██║ ██║   ██║ ██║╚██╗██║    ██║    ██║ ██║╚██╗██║ ██║   ██║ \n",
    "#╚██████╗ ╚██████╔╝ ╚██████╔╝ ██║ ╚████║    ██║    ██║ ██║ ╚████║ ╚██████╔╝ \n",
    "# ╚═════╝  ╚═════╝   ╚═════╝  ╚═╝  ╚═══╝    ╚═╝    ╚═╝ ╚═╝  ╚═══╝  ╚═════╝  \n",
    "# As we have binned all the data into serveral differnt groups, we now need to count how many\n",
    "# are in each group\n",
    "\n",
    "#creates an empty array\n",
    "dfc = {}\n",
    "start_time2 = time.perf_counter()\n",
    "\n",
    "#for each year\n",
    "for year in years:\n",
    "    dfc[year] = {}\n",
    "    \n",
    "    #for each month\n",
    "    for month in range(covid_started[0], months_in_year + 1): \n",
    "        dfc[year][month] = {}\n",
    "        \n",
    "        #for each taxi service\n",
    "        for taxi_i in range(0,len(taxi_file_names)):\n",
    "            \n",
    "            #clears array for data count\n",
    "            dfc[year][month][taxi_i] = None\n",
    "            \n",
    "            #if the data frame is not nothing\n",
    "            if( dfm[year][month][taxi_i] ):\n",
    "                \n",
    "                #prints what stage count it up to\n",
    "                print(\"counting \" + str(year) + \"-\" + str(month)+\" \"+taxi_file_names[taxi_i])\n",
    "            \n",
    "                #prints a dataframe of the count groupped by the binning columns\n",
    "                dfc[year][month][taxi_i] = dfm[year][month][taxi_i]\\\n",
    "                    .groupBy(\"week_of_year\",\"day_of_week\",\"PU_borough_code\",\"DO_borough_code\",\"part_of_day\")\\\n",
    "                    .count()\\\n",
    "                    .toPandas()\n",
    "                \n",
    "                #doesnt save data if its just a test (otherwise would overwrite good data)\n",
    "                if not testing:\n",
    "                    #lost a lot of data mid way once so added this safe guard\n",
    "                    dfc[year][month][taxi_i].to_csv(processed_dir + taxi_file_names[taxi_i] \n",
    "                                                    + processed_data_file + str(year) + \"-\" \n",
    "                                                    + str(month).zfill(2) + \".csv\") \n",
    "            \n",
    "                #prints how much time has elapsed\n",
    "                print(str(time.perf_counter() - start_time2) + \" seconds have elapsed\")\n",
    "                print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "                start_time2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.831743499998993 seconds have elapsed\n",
      "45.83847679999599 seconds have elapsed total\n"
     ]
    }
   ],
   "source": [
    "#prints how much time has elapsed\n",
    "print(str(time.perf_counter() - start_time) + \" seconds have elapsed\")\n",
    "print(str(time.perf_counter() - start_time_og) + \" seconds have elapsed total\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2019: {2: {0: None,\n",
       "   1:      week_of_year  day_of_week  PU_borough_code  DO_borough_code  part_of_day  \\\n",
       "   0               5            1                0                0            3   \n",
       "   1               4            6                1                2            3   \n",
       "   2               5            0                3                2            1   \n",
       "   3               4            5                1                1            0   \n",
       "   4               5            0                0                2            0   \n",
       "   ..            ...          ...              ...              ...          ...   \n",
       "   246             4            6                3                3            0   \n",
       "   247             4            6                1                3            3   \n",
       "   248             5            2                3                3            1   \n",
       "   249             5            2                3                1            1   \n",
       "   250             5            2                3                2            2   \n",
       "   \n",
       "        count  \n",
       "   0       10  \n",
       "   1       33  \n",
       "   2        9  \n",
       "   3        6  \n",
       "   4        2  \n",
       "   ..     ...  \n",
       "   246     33  \n",
       "   247      7  \n",
       "   248     41  \n",
       "   249      3  \n",
       "   250      3  \n",
       "   \n",
       "   [251 rows x 6 columns],\n",
       "   2:     week_of_year  day_of_week  PU_borough_code  DO_borough_code  part_of_day  \\\n",
       "   0              4            6                4                4            0   \n",
       "   1              4            5                1                1            0   \n",
       "   2              4            5                2                4            2   \n",
       "   3              4            6                2                2            0   \n",
       "   4              4            5                0                2            1   \n",
       "   ..           ...          ...              ...              ...          ...   \n",
       "   79             4            6                3                3            2   \n",
       "   80             4            6                4                4            2   \n",
       "   81             4            5                2                0            1   \n",
       "   82             4            5                2                3            1   \n",
       "   83             4            6                3                3            0   \n",
       "   \n",
       "       count  \n",
       "   0       1  \n",
       "   1       3  \n",
       "   2       1  \n",
       "   3       1  \n",
       "   4       7  \n",
       "   ..    ...  \n",
       "   79     27  \n",
       "   80      2  \n",
       "   81      1  \n",
       "   82      1  \n",
       "   83     34  \n",
       "   \n",
       "   [84 rows x 6 columns],\n",
       "   3: None}},\n",
       " 2020: {2: {0: None,\n",
       "   1:      week_of_year  day_of_week  PU_borough_code  DO_borough_code  part_of_day  \\\n",
       "   0               5            1                0                0            3   \n",
       "   1               5            4                1                0            3   \n",
       "   2               4            6                1                2            3   \n",
       "   3               5            0                3                2            1   \n",
       "   4               5            2                1                1            3   \n",
       "   ..            ...          ...              ...              ...          ...   \n",
       "   354             5            2                3                1            1   \n",
       "   355             5            6                2                2            1   \n",
       "   356             5            2                3                2            2   \n",
       "   357             5            3                1                1            1   \n",
       "   358             5            6                3                2            1   \n",
       "   \n",
       "        count  \n",
       "   0        2  \n",
       "   1        1  \n",
       "   2        9  \n",
       "   3        1  \n",
       "   4       55  \n",
       "   ..     ...  \n",
       "   354      2  \n",
       "   355     46  \n",
       "   356      1  \n",
       "   357     25  \n",
       "   358      3  \n",
       "   \n",
       "   [359 rows x 6 columns],\n",
       "   2:      week_of_year  day_of_week  PU_borough_code  DO_borough_code  part_of_day  \\\n",
       "   0               4            6                1                2            3   \n",
       "   1               5            0                3                2            1   \n",
       "   2               4            6                4                4            0   \n",
       "   3               4            6                2                4            3   \n",
       "   4               4            6                2                2            0   \n",
       "   ..            ...          ...              ...              ...          ...   \n",
       "   146             5            1                2                2            1   \n",
       "   147             5            0                3                3            2   \n",
       "   148             4            6                3                3            0   \n",
       "   149             4            6                1                3            3   \n",
       "   150             5            0                3                1            0   \n",
       "   \n",
       "        count  \n",
       "   0        3  \n",
       "   1        2  \n",
       "   2        5  \n",
       "   3        1  \n",
       "   4        3  \n",
       "   ..     ...  \n",
       "   146      8  \n",
       "   147     78  \n",
       "   148     32  \n",
       "   149      8  \n",
       "   150      2  \n",
       "   \n",
       "   [151 rows x 6 columns],\n",
       "   3: None}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prints the current output array to see it looks alright\n",
    "dfc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
